{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8786c118-54ab-4ac1-9309-0c49663971e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from lime import lime_tabular\n",
    "from lime import submodular_pick\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "\n",
    "# Check loops\n",
    "def checkLoops(expName):\n",
    "    data = ds.loc[:,['caseId', 'activity']]\n",
    "\n",
    "    actCount = data.groupby(data.columns.tolist()).size().reset_index().\\\n",
    "                        rename(columns={0:'records'})\n",
    "    print(actCount.describe())\n",
    "    #print(actCount[:20])\n",
    "    \n",
    "def checkCorrelation():\n",
    "    ds['Accept Claim'] = ds['Accept Claim'].astype(bool)\n",
    "    print('Accept == label (1)', ds['Accept Claim'].equals(ds['label']))\n",
    "\n",
    "    ds['Reject Claim'] = ds['Reject Claim'].astype(bool)\n",
    "    print('Reject == label (0)', ds['Reject Claim'].equals(~ds['label']))\n",
    "    \n",
    "# Delete out loops, Input: trace (lista), Ouput: traceSemLoop (lista)\n",
    "def dropLoopsInTraces(traceList):\n",
    "    prev = object()\n",
    "    traceList = [prev := v for v in traceList if prev != v]\n",
    "    return traceList\n",
    "\n",
    "# Transform the log into dataset of traces\n",
    "def f_Traces(x):\n",
    "    return pd.Series(dict(trace='%s' % ','.join(x['activity']),\n",
    "                      nrEvents=x['activity'].count(),\n",
    "                      target=int(x[target].mean())))\n",
    "\n",
    "# Transform the log into dataset of traces considering resources\n",
    "def f_TracesComplex(x):\n",
    "    return pd.Series(dict(tracea='%s' % ','.join(x['activity']),\n",
    "                          tracer='%s' % ','.join(x['Resource']),\n",
    "                          nrEvents=x['activity'].count(),\n",
    "                          target=int(x[target].mean())))\n",
    "\n",
    "# Load event log data\n",
    "def loadData(data, column_caseId, column_activity):   \n",
    "    global ds, target, class_names, expName\n",
    "    ds = pd.read_csv('data/' + data.split('_')[0] + '.csv', sep=',')\n",
    "    target = 'label'\n",
    "    class_names = [0, 1]\n",
    "    ds = ds.rename(columns={column_caseId: \"caseId\", column_activity: \"activity\"})\n",
    "    expName = data\n",
    "    print(\"Data loaded...\")\n",
    "\n",
    "# Pre-process data: simple indexing, considering trace positions,  Input: dataset of traces (dataframe), Ouput: dataset of activity per position in trace (dataframe)\n",
    "def simpleIndexingNoLoops():\n",
    "    global ds\n",
    "    ds = ds.groupby('caseId').apply(f_Traces)\n",
    "    ds_new = pd.DataFrame(columns=['caseId', 'trace', 'nrEvents', target])\n",
    "    for item in ds.iterrows():\n",
    "        noduplicates = dropLoopsInTraces(item[1][0].split(','))\n",
    "        ds_new.loc[len(ds_new)] = [int(item[0]), ','.join(noduplicates), len(noduplicates), int(item[1][2])]\n",
    "\n",
    "    ds_new[['a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', 'a9', 'a10', 'a11', 'a12', 'a13']] = ds_new['trace'].str.split(',', expand=True)\n",
    "    ds_new = ds_new.drop(['caseId', 'trace', 'nrEvents'], axis=1)\n",
    "    ds_new.to_csv('results/' + expName + '_traces.csv')\n",
    "    ds = ds_new.copy()\n",
    "    print(\"Data preprocessed... SIMPLE-INDEXING ENCODING\")\n",
    "\n",
    "    \n",
    "# Pre-process data: frequency indexing, considering frequency of occurrence of an activity in the trace\n",
    "def frequencyIndexingWithLoops():\n",
    "    global ds\n",
    "    ds.loc[ds.activity =='Accept Claim','activity']='Decide claim'\n",
    "    ds.loc[ds.activity =='Reject Claim','activity']='Decide claim'\n",
    "    ds_new = pd.DataFrame()\n",
    "    ds_new[['caseId', 'activity','label']] = ds[['caseId', 'activity','label']].copy()\n",
    "    ds_new['val'] = 1\n",
    "    ds_new = ds_new.groupby(['caseId', 'label', 'activity'])['val'].sum().unstack(fill_value=0)\n",
    "    ds_new.reset_index(level = 'label', inplace=True)\n",
    "    ds_new.reset_index(level = 'caseId', inplace=True)\n",
    "    ds_new.drop(['caseId'], axis = 1, inplace=True)\n",
    "    ds_new.to_csv('results/' + expName + '_frequencytraces.csv')\n",
    "    ds = ds_new.copy()\n",
    "    print(\"Data preprocessed... FREQUENCY-INDEXING ENCODING\")\n",
    "\n",
    "# Apply one-hot encoding and split data\n",
    "def encodingAndSplitData():\n",
    "    global X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "    X = pd.get_dummies(ds.drop(columns=[target]))\n",
    "    y = ds[target].astype(bool)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Set the random_state parameter to 42 for getting the same split\n",
    "    X_train = X\n",
    "    X_test = X\n",
    "    y_train = y\n",
    "    y_test = y\n",
    "    X_train.to_csv(\"results/\"+ expName + \"_XTrain.csv\")\n",
    "    X_test.to_csv(\"results/\"+ expName + \"_XTest.csv\")\n",
    "    print(\"Data splitted...\")\n",
    "    print('X_train shape: ', X_train.shape)\n",
    "    print('y_train shape: ', y_train.shape)\n",
    "    print('X_test shape: ', X_test.shape)\n",
    "    print('y_test shape: ', y_test.shape)\n",
    "    \n",
    "# Train um RandomForestClassifier from ScikitLearn\n",
    "def trainRFModel():\n",
    "    global model\n",
    "    model = RandomForestClassifier(random_state = np.random.seed(42))\n",
    "    model.fit(X_train, y_train)\n",
    "    #Return the mean accuracy on the given test data and label\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Model trained...')\n",
    "    print('Score (mean accuracy): ', score)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc=metrics.accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy (metrics) is:', acc)\n",
    "          \n",
    "\n",
    "# Apply Lime for an instance, Input: CaseId (int), Output: LimeExp (lime object)\n",
    "def applyLimeIn(points, nrFeatures):\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "                                                    training_data=np.array(X_train),\n",
    "                                                    feature_names=X_train.columns,\n",
    "                                                    class_names=class_names,\n",
    "                                                    mode='classification')\n",
    "\n",
    "    exp_points = []\n",
    "    exp_list = []\n",
    "\n",
    "    for idx in points:\n",
    "        exp = explainer.explain_instance(data_row=X_train.loc[idx], predict_fn=model.predict_proba, num_features=nrFeatures)\n",
    "        exp_list = exp.as_list()\n",
    "        exp_list.append(round(model.predict_proba([X_train.loc[idx]])[0, 0], 2))\n",
    "        exp_list.append(round(model.predict_proba([X_train.loc[idx]])[0, 1], 2))\n",
    "        exp_list.append(ds.loc[idx, target]) #exp_list.append(ds_new.loc[idx, target])\n",
    "        exp_list.append(idx)\n",
    "        exp_points.append(exp_list)\n",
    "    fileName = \"results/\"+ expName + \"_explanations.csv\"\n",
    "    with open(fileName, \"w\") as file:\n",
    "        for row in exp_points:\n",
    "            file.write(\"%s\\n\" % ';'.join(str(col) for col in row))\n",
    "    print(\"Explanations file created... \" + fileName)\n",
    "\n",
    "    \n",
    "# Apply SP-Lime\n",
    "def applySPLimeIn(sampleSize, nrFeatures, nrExplanations):\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "                                                    training_data=np.array(X_train),\n",
    "                                                    feature_names=X_train.columns,\n",
    "                                                    class_names=class_names,\n",
    "                                                    mode='classification')\n",
    "\n",
    "    training_data=np.array(X_train)\n",
    "    sp_obj = submodular_pick.SubmodularPick(explainer, data=training_data, predict_fn=model.predict_proba, sample_size=sampleSize, num_features=nrFeatures, num_exps_desired=nrExplanations)\n",
    "    exp_points = sp_obj.sp_explanations[9].as_list()\n",
    "    fileName = \"results/\"+ expName + \"_explSPLIME.csv\"\n",
    "    with open(fileName, \"w\") as file:\n",
    "        for row in exp_points:\n",
    "            file.write(\"%s\\n\" % ';'.join(str(col) for col in row))\n",
    "    print(\"Explanations with SP-LIME file created... \"+fileName)\n",
    "\n",
    "# Plot results from file generated by the LIME module, Input: Number of figures per row (int), Experiment Name (str)\n",
    "def plotLimeResults(plotsPerRow, expName):        \n",
    "    i, j = 0, 0\n",
    "    fileName = \"results/\"+ expName + \"_explLime.csv\"\n",
    "    with open(fileName, \"r\") as file:\n",
    "        exp_points = list(file)\n",
    "    exp_points = [x.rstrip() for x in exp_points]\n",
    "    exp_points = [list(x.split(';')) for x in exp_points]\n",
    "\n",
    "    # Graph multiplot\n",
    "    fig, axs = plt.subplots(nrows=math.ceil(len(exp_points) / plotsPerRow), ncols=plotsPerRow, constrained_layout=True, figsize=(20, 20))  # #squeeze=False, you can force the result to be a 2D-array, independant of the number or arrangement of the subplots\n",
    "    fig.suptitle('Experiment: %s' % expName, fontsize=20)  # title for entire figure\n",
    "\n",
    "    exp_list = []\n",
    "    for exp_list in exp_points:\n",
    "        exp_list = [x.strip('(') for x in exp_list]\n",
    "        exp_list = [x.strip(')') for x in exp_list]\n",
    "        exp_list = [x.split(', ') for x in exp_list]\n",
    "        names = [x[0].strip(\"\\'\") for x in exp_list[:-4]]  # Y\n",
    "        names = [n.rpartition('.0')[0] for n in names]\n",
    "        vals = [round(float(x[1]), 3) for x in exp_list[:-4]]  # X\n",
    "        pos = np.arange(len(exp_list) - 4) + .5\n",
    "        prob0 = exp_list[len(exp_list) - 4][0]\n",
    "        prob1 = exp_list[len(exp_list) - 3][0]\n",
    "        y_target = exp_list[len(exp_list) - 2][0]\n",
    "        idx = exp_list[len(exp_list) - 1][0]\n",
    "        vals.reverse()\n",
    "        names.reverse()\n",
    "        colors = ['green' if x > 0 else 'red' for x in vals]\n",
    "        axs[i][j].set_title('Case Id: %s\\nProbability for class 0 = %s \\nProbability for class 1 = %s\\n Target/Y: %s' % (idx,\n",
    "                                                                                                                      str(prob0),\n",
    "                                                                                                                      str(prob1),\n",
    "                                                                                                                      y_target))\n",
    "        axs[i][j].barh(pos, vals, align='center', color=colors)\n",
    "        axs[i][j].set_yticks(pos, names)\n",
    "        j += 1\n",
    "        if j % plotsPerRow == 0:\n",
    "            i += 1\n",
    "            j = 0\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(str(\"results/\" + expName + \"_plotLime.jpg\"), bbox_inches='tight')\n",
    "    print('Figure saved...' + \"results/\" + expName + \"_plotLime.jpg\")\n",
    "\n",
    "# plot LIME-SP results in a a global way ***not working yet***\n",
    "def plotLimeSPResults(sp_obj):\n",
    "    # Plot all explanations\n",
    "    #[explainer.as_pyplot_figure(label=explainer.available_labels()[0]) for explainer in sp_obj.sp_explanations];\n",
    "    i=0\n",
    "    for explainer in sp_obj.sp_explanations:\n",
    "        fig = explainer.as_pyplot_figure(label=explainer.available_labels()[0])        \n",
    "        fig.savefig(str(\"results/\" + expName + \"_SPLime\"+str(i)+\".jpg\"), bbox_inches='tight')\n",
    "        i += 1\n",
    "    \n",
    "    \n",
    "    # Make it into a dataframe SP-LIME\n",
    "    W_pick=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.sp_explanations]).fillna(0)\n",
    "\n",
    "    # Getting SP predictions\n",
    "    W_pick['_Prediction <= ----'] = [this.available_labels()[0] for this in sp_obj.sp_explanations]\n",
    "\n",
    "    W_pick.columns = formatColumns(W_pick.columns)\n",
    "    W_pick.sort_index(axis=1, inplace = True)\n",
    "\n",
    "    W_pick.to_csv(\"results/\"+ expName + \"_WmatrixSPLime.csv\")\n",
    "    print('Results saved...' + \"results/\" + expName + \"_WmatrixSPLime.jpg\")\n",
    "\n",
    "    #Making a dataframe of all the explanations of sampled points SIMPLE - LIME\n",
    "    W=pd.DataFrame([dict(this.as_list(this.available_labels()[0])) for this in sp_obj.explanations]).fillna(0)\n",
    "    W['prediction'] = [this.available_labels()[0] for this in sp_obj.explanations]\n",
    "    W.columns = formatColumns(W.columns)\n",
    "    W.sort_index(axis=1, inplace = True)\n",
    "    W.to_csv(\"results/\"+ expName + \"_WfullLime.csv\")\n",
    "    print('Results saved...' + \"results/\" + expName + \"_WfullLime.csv\")\n",
    "    \n",
    "def formatColumns(listColumnsName):\n",
    "    names = []\n",
    "    i=0\n",
    "    for name in listColumnsName:\n",
    "        if name[0] == '0':\n",
    "            name = name[7:]    \n",
    "        names.append(name[:-8]+'='+name[len(name)-4:-3])\n",
    "        i=+ 1\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca56a2fa-b2f0-4baa-a4d8-e112da4ba5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainMarcelo_semdup16\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/trainMarcelo.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13732\\1896728406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mds2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexpName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data loaded...16\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trainMarcelo.csv'"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "# EXPERIMETS ARTICLE 16\n",
    "####################################\n",
    "expName = 'trainMarcelo_semdup16'\n",
    "print(expName)\n",
    "\n",
    "\n",
    "ds2 = pd.read_csv('data/' + expName.split('_')[0] + '.csv', sep=',')\n",
    "print(\"Data loaded...16\")\n",
    "\n",
    "# Select type of modeling for traces in log\n",
    "#frequencyIndexingWithLoops()\n",
    "\n",
    "X = pd.get_dummies(ds2.drop(columns=[target]))\n",
    "y = ds2[target].astype(bool)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Set the random_state parameter to 42 for getting the same split\n",
    "X_train = X\n",
    "X_test = X\n",
    "y_train = y\n",
    "y_test = y\n",
    "X_train.to_csv(\"results/\"+ expName + \"_XTrain.csv\")\n",
    "X_test.to_csv(\"results/\"+ expName + \"_XTest.csv\")\n",
    "print(\"Data splitted...\")\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)\n",
    "# Train a ML model\n",
    "#encodingAndSplitData()\n",
    "#trainRFModel()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "modelknn = GridSearchCV(knn, params, cv=5)\n",
    "modelknn.fit(X_train,y_train)\n",
    "print('KNN trained...')\n",
    "\n",
    "print('GridSearchCV KNN k ---> ', modelknn.best_params_)\n",
    "\n",
    "y_pred=modelknn.predict(X_test) #make prediction on test set\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "print('acc knn ', acc)\n",
    "\n",
    "model = RandomForestClassifier(random_state = np.random.seed(42))\n",
    "model.fit(X_train, y_train)\n",
    "#Return the mean accuracy on the given test data and label\n",
    "score = model.score(X_test, y_test)\n",
    "print('RF trained...')\n",
    "\n",
    "#print('Score (mean accuracy): ', score)\n",
    "y_pred = model.predict(X_test)\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "print('Accuracy (metrics) is:', acc)\n",
    "          \n",
    "y_pred=model.predict(X_test) #make prediction on test set\n",
    "acc=metrics.accuracy_score(y_test, y_pred)\n",
    "print('acc RF ', acc)\n",
    "\n",
    "# SP-LIME\n",
    "\n",
    "training_dataSPLime=np.array(X_train)\n",
    "\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "                                                    training_data=training_dataSPLime,\n",
    "                                                    feature_names=X_train.columns,\n",
    "                                                    class_names=class_names,\n",
    "                                                    mode='classification')\n",
    "\n",
    "\n",
    "sp_obj = submodular_pick.SubmodularPick(explainer, data=training_dataSPLime, predict_fn=modelknn.predict_proba, num_features=20, num_exps_desired=10)\n",
    "plotLimeSPResults(sp_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
